{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!python -m pip install --upgrade cityscapesscripts[gui] timm torchvision transformers appdirs requests datasets nvidia-dali-cuda120 torch==2.5.1 triton"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6BFBeIF23AC","executionInfo":{"status":"ok","timestamp":1739854923500,"user_tz":300,"elapsed":112756,"user":{"displayName":"Daniel Hardesty Lewis","userId":"13406551434522648204"}},"outputId":"5fa85244-4541-479c-de43-1a9ae2415873"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.14)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n","Collecting torchvision\n","  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Collecting transformers\n","  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting appdirs\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Collecting datasets\n","  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n","Collecting nvidia-dali-cuda120\n","  Downloading nvidia_dali_cuda120-1.46.0.tar.gz (1.6 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.1.0)\n","Collecting triton\n","  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting cityscapesscripts[gui]\n","  Downloading cityscapesScripts-2.2.4-py3-none-any.whl.metadata (9.8 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cityscapesscripts[gui]) (1.26.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from cityscapesscripts[gui]) (3.10.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cityscapesscripts[gui]) (11.1.0)\n","Collecting pyquaternion (from cityscapesscripts[gui])\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n","Collecting coloredlogs (from cityscapesscripts[gui])\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cityscapesscripts[gui]) (4.67.1)\n","Collecting typing (from cityscapesscripts[gui])\n","  Downloading typing-3.7.4.3.tar.gz (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting PyQt5 (from cityscapesscripts[gui])\n","  Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.28.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.2)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n","Requirement already satisfied: astunparse<=1.6.3,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-dali-cuda120) (1.6.3)\n","Requirement already satisfied: gast<=0.6.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from nvidia-dali-cuda120) (0.6.0)\n","Collecting six<=1.16,>=1.16 (from nvidia-dali-cuda120)\n","  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Collecting dm-tree<=0.1.8 (from nvidia-dali-cuda120)\n","  Downloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n","Collecting nvidia-nvjpeg2k-cu12<0.9.0,>=0.8.0 (from nvidia-dali-cuda120)\n","  Downloading nvidia_nvjpeg2k_cu12-0.8.1.40-py3-none-manylinux2014_x86_64.whl.metadata (2.2 kB)\n","Collecting nvidia-nvtiff-cu12<0.5.0,>=0.4.0 (from nvidia-dali-cuda120)\n","  Downloading nvidia_nvtiff_cu12-0.4.0.62-py3-none-manylinux2014_x86_64.whl.metadata (2.1 kB)\n","Collecting nvidia-nvimgcodec-cu12<0.5.0,>=0.4.1 (from nvidia-dali-cuda120)\n","  Downloading nvidia_nvimgcodec_cu12-0.4.1.21-py3-none-manylinux2014_x86_64.whl.metadata (813 bytes)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse<=1.6.3,>=1.6.0->nvidia-dali-cuda120) (0.45.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Collecting humanfriendly>=9.1 (from coloredlogs->cityscapesscripts[gui])\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesscripts[gui]) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesscripts[gui]) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesscripts[gui]) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesscripts[gui]) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesscripts[gui]) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesscripts[gui]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Collecting PyQt5-sip<13,>=12.15 (from PyQt5->cityscapesscripts[gui])\n","  Downloading PyQt5_sip-12.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (472 bytes)\n","Collecting PyQt5-Qt5<5.16.0,>=5.15.2 (from PyQt5->cityscapesscripts[gui])\n","  Downloading PyQt5_Qt5-5.15.16-py3-none-manylinux2014_x86_64.whl.metadata (536 bytes)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Downloading datasets-3.3.1-py3-none-any.whl (484 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvimgcodec_cu12-0.4.1.21-py3-none-manylinux2014_x86_64.whl (23.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjpeg2k_cu12-0.8.1.40-py3-none-manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtiff_cu12-0.4.0.62-py3-none-manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Downloading cityscapesScripts-2.2.4-py3-none-any.whl (473 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.6/473.6 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl (8.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyQt5_Qt5-5.15.16-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyQt5_sip-12.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl (276 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: nvidia-dali-cuda120, typing\n","  Building wheel for nvidia-dali-cuda120 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvidia-dali-cuda120: filename=nvidia_dali_cuda120-1.46.0-py3-none-manylinux2014_x86_64.whl size=409238405 sha256=7a497d68d3d05292b8644e75cdf3418f6ef02fcfb515e65e23da0cef4251130e\n","  Stored in directory: /root/.cache/pip/wheels/98/9e/25/71fed3123336acc0921ac2fe2ed62420e2b83a92d76df6426d\n","  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26303 sha256=2a868c2fb94fde9cbb064b593a76f8a40d4e006874f9573bf9997cc9e2996c0e\n","  Stored in directory: /root/.cache/pip/wheels/9d/67/2f/53e3ef32ec48d11d7d60245255e2d71e908201d20c880c08ee\n","Successfully built nvidia-dali-cuda120 typing\n","Installing collected packages: PyQt5-Qt5, dm-tree, appdirs, xxhash, typing, six, pyquaternion, PyQt5-sip, nvidia-nvtiff-cu12, nvidia-nvjpeg2k-cu12, nvidia-nvjitlink-cu12, nvidia-nvimgcodec-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, dill, PyQt5, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, coloredlogs, nvidia-dali-cuda120, nvidia-cusolver-cu12, transformers, datasets, cityscapesscripts\n","  Attempting uninstall: dm-tree\n","    Found existing installation: dm-tree 0.1.9\n","    Uninstalling dm-tree-0.1.9:\n","      Successfully uninstalled dm-tree-0.1.9\n","  Attempting uninstall: six\n","    Found existing installation: six 1.17.0\n","    Uninstalling six-1.17.0:\n","      Successfully uninstalled six-1.17.0\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.48.3\n","    Uninstalling transformers-4.48.3:\n","      Successfully uninstalled transformers-4.48.3\n","Successfully installed PyQt5-5.15.11 PyQt5-Qt5-5.15.16 PyQt5-sip-12.17.0 appdirs-1.4.4 cityscapesscripts-2.2.4 coloredlogs-15.0.1 datasets-3.3.1 dill-0.3.8 dm-tree-0.1.8 humanfriendly-10.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-dali-cuda120-1.46.0 nvidia-nvimgcodec-cu12-0.4.1.21 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvjpeg2k-cu12-0.8.1.40 nvidia-nvtiff-cu12-0.4.0.62 pyquaternion-0.9.9 six-1.16.0 transformers-4.49.0 typing-3.7.4.3 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python\n","\"\"\"\n","SEGMENTER: More Official-Like Training Script with GPU-Accelerated Augmentation via NVIDIA DALI\n","and Automatic Resource-Based Tuning\n","\n","This script reproduces the \"Segmenter\" paper's approach while:\n","  - Detecting system CPU cores and available RAM to automatically tune the DataLoader.\n","  - Checking GPU memory and printing a summary so you can adjust batch size if desired.\n","  - Using channels_last, AMP, and torch.compile (if available) for improved GPU throughput.\n","  - Optionally using Distributed Data Parallel (DDP) for multi-GPU training.\n","  - Optionally using NVIDIA DALI for GPU-accelerated data loading & augmentation.\n","\n","It supports large or small datasets (ADE20K, Cityscapes, Pascal Context, or a toy dataset),\n","and includes standard or advanced augmentations (Mixup, CutMix, ColorJitter, etc.).\n","See the argument list for details.\n","\n","Example usage (with DALI, on ADE20K):\n","python segmenter_official_like.py \\\n","    --dataset_name ade20k \\\n","    --num_classes 150 \\\n","    --lr 0.001 \\\n","    --total_iterations 161728 \\\n","    --batch_size 16 \\\n","    --grad_accum_steps 1 \\\n","    --eval_freq 2 \\\n","    --num_epochs 64 \\\n","    --start_epoch 0 \\\n","    --model_name vit_base_patch16_384 \\\n","    --decoder mask \\\n","    --stoch_depth 0.1 \\\n","    --backbone_dropout 0.0 \\\n","    --decoder_drop_path_rate 0.0 \\\n","    --advanced_aug \\\n","    --cutmix \\\n","    --dataset_normalization vit \\\n","    --iter_warmup 0.0 \\\n","    --min_lr 1e-05 \\\n","    --clip_grad 0.0 \\\n","    --cache_data \\\n","    --use_dali \\\n","    --distilled false \\\n","    --version normal \\\n","    [--ddp]\n","\n","NOTE:\n","  - If using multiple GPUs, run with torch.distributed launch or torchrun to enable --ddp.\n","  - For GCP or Colab usage, set the runtime to your desired accelerator (GPU/TPU).\n","\"\"\"\n","\n","import os\n","import sys\n","import json\n","import stat\n","import tarfile\n","import getpass\n","import requests\n","import appdirs\n","import math\n","import numpy as np\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms as T\n","import timm  # Vision Transformers, etc.\n","\n","# Allow PIL to load truncated images\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","# Enable cuDNN benchmarking if input sizes are fixed\n","torch.backends.cudnn.benchmark = True\n","\n","########################################\n","# 0) Automatic Hardware Detection\n","########################################\n","try:\n","    import psutil\n","    cpu_logical = psutil.cpu_count(logical=True)\n","    cpu_physical = psutil.cpu_count(logical=False)\n","    mem = psutil.virtual_memory()\n","    free_ram_gb = mem.available / (1024**3)\n","    print(f\"Detected CPU: {cpu_logical} logical cores, {cpu_physical} physical cores.\")\n","    print(f\"Available system RAM: {free_ram_gb:.1f} GB\")\n","except ImportError:\n","    print(\"psutil not found. Install psutil for CPU/RAM detection.\")\n","    cpu_logical = 4\n","    cpu_physical = 2\n","    free_ram_gb = 8\n","\n","########################################\n","# 1) Download/Fetch Datasets (Illustrative Examples)\n","########################################\n","def login_cityscapes():\n","    \"\"\"Logs in to cityscapes-dataset.com. Prompts user if missing credentials.\"\"\"\n","    appname = \"cityscapesLoginApp\"\n","    appauthor = \"cityscapes\"\n","    data_dir = appdirs.user_data_dir(appname, appauthor)\n","    credentials_file = os.path.join(data_dir, 'credentials.json')\n","    if os.path.isfile(credentials_file):\n","        with open(credentials_file, 'r') as f:\n","            credentials = json.load(f)\n","    else:\n","        username = input(\"Cityscapes username/email: \")\n","        password = getpass.getpass(\"Cityscapes password: \")\n","        credentials = {'username': username, 'password': password}\n","        store_q = f\"Store credentials unencrypted in '{credentials_file}'? [y/N] \"\n","        if input(store_q).strip().lower() in ['y', 'yes']:\n","            os.makedirs(data_dir, exist_ok=True)\n","            with open(credentials_file, 'w') as f:\n","                json.dump(credentials, f)\n","            os.chmod(credentials_file, stat.S_IREAD | stat.S_IWRITE)\n","    sess = requests.Session()\n","    r = sess.get(\"https://www.cityscapes-dataset.com/login\", allow_redirects=False)\n","    r.raise_for_status()\n","    credentials['submit'] = 'Login'\n","    r2 = sess.post(\"https://www.cityscapes-dataset.com/login\", data=credentials, allow_redirects=False)\n","    r2.raise_for_status()\n","    if r2.status_code != 302:\n","        if os.path.isfile(credentials_file):\n","            os.remove(credentials_file)\n","        raise Exception(\"Cityscapes login failed. Check credentials.\")\n","    return sess\n","\n","def parse_size_to_bytes(size_str):\n","    \"\"\"Convert e.g. '12MB' => 12582912 bytes.\"\"\"\n","    size_str = size_str.strip().upper()\n","    if size_str.endswith(\"KB\"):\n","        return float(size_str[:-2]) * 1024\n","    elif size_str.endswith(\"MB\"):\n","        return float(size_str[:-2]) * 1024 * 1024\n","    elif size_str.endswith(\"GB\"):\n","        return float(size_str[:-2]) * 1024 * 1024 * 1024\n","    else:\n","        raise ValueError(f\"Unknown size format '{size_str}' in parse_size_to_bytes\")\n","\n","def get_cityscapes_packages(session):\n","    r = session.get(\"https://www.cityscapes-dataset.com/downloads/?list\", allow_redirects=False)\n","    r.raise_for_status()\n","    return r.json()\n","\n","def download_cityscapes_packages(pkg_names, destination_path, session, resume=False):\n","    \"\"\"Download official cityscapes zips with optional resume.\"\"\"\n","    pkgs_info = get_cityscapes_packages(session)\n","    name_to_id = {p['name']: p['packageID'] for p in pkgs_info}\n","    name_to_bytes = {p['name']: parse_size_to_bytes(p['size']) for p in pkgs_info}\n","    # Validate\n","    invalid = [n for n in pkg_names if n not in name_to_id]\n","    if invalid:\n","        raise ValueError(f\"Package(s) not recognized: {invalid}\")\n","    os.makedirs(destination_path, exist_ok=True)\n","\n","    for pkg in pkg_names:\n","        pkg_id = name_to_id[pkg]\n","        out_path = os.path.join(destination_path, pkg)\n","        # Check if existing\n","        if os.path.exists(out_path) and not resume:\n","            raise FileExistsError(f\"{out_path} exists. Pass resume=True to continue partial.\")\n","        # Get MD5\n","        md5_url = f\"https://www.cityscapes-dataset.com/md5-sum/?packageID={pkg_id}\"\n","        r_md5 = session.get(md5_url, allow_redirects=False)\n","        r_md5.raise_for_status()\n","        md5_expected = r_md5.text.split()[0]\n","\n","        # Download\n","        data_url = f\"https://www.cityscapes-dataset.com/file-handling/?packageID={pkg_id}\"\n","        mode = 'ab' if resume else 'wb'\n","        resume_header = {}\n","        start_offset = 0\n","        if resume and os.path.exists(out_path):\n","            start_offset = os.path.getsize(out_path)\n","            resume_header = {'Range': f'bytes={start_offset}-'}\n","        r_data = session.get(data_url, allow_redirects=False, stream=True, headers=resume_header)\n","        r_data.raise_for_status()\n","        assert r_data.status_code in [200, 206], f\"Got {r_data.status_code} from server.\"\n","\n","        total_size = name_to_bytes[pkg]\n","        with open(out_path, mode) as f_out, tqdm(\n","            desc=f\"Download {pkg}\", total=total_size,\n","            initial=start_offset, unit='B', unit_scale=True\n","        ) as pbar:\n","            for chunk in r_data.iter_content(chunk_size=8192):\n","                f_out.write(chunk)\n","                pbar.update(len(chunk))\n","        # Check MD5\n","        import hashlib\n","        calc_md5 = hashlib.md5()\n","        with open(out_path, \"rb\") as fch:\n","            for chunk in iter(lambda: fch.read(4096), b\"\"):\n","                calc_md5.update(chunk)\n","        if calc_md5.hexdigest() != md5_expected:\n","            raise ValueError(f\"MD5 mismatch for {pkg} => corrupted download.\")\n","\n","def fetch_cityscapes_official(root_dir=\"datasets/Cityscapes_official\"):\n","    \"\"\"Automate Cityscapes official data fetch (requires credentials).\"\"\"\n","    left_dir = os.path.join(root_dir, \"leftImg8bit\")\n","    gt_dir = os.path.join(root_dir, \"gtFine\")\n","    if os.path.isdir(left_dir) and os.path.isdir(gt_dir):\n","        print(\"[Cityscapes] Found => skipping download.\")\n","        return root_dir\n","    print(\"[Cityscapes] Downloading official zips.\")\n","    sess = login_cityscapes()\n","    desired = [\"leftImg8bit_trainvaltest.zip\", \"gtFine_trainvaltest.zip\"]\n","    download_cityscapes_packages(desired, root_dir, sess, resume=False)\n","    print(\"[Cityscapes] Download complete. Please unzip these files in place. The final folder should have leftImg8bit/, gtFine/.\")\n","    return root_dir\n","\n","def fetch_voc2007_official(root_dir=\"datasets/VOC2007_official\"):\n","    voc_root = os.path.join(root_dir, \"VOCdevkit\", \"VOC2007\")\n","    success_file = os.path.join(voc_root, \"_SUCCESS.txt\")\n","    if os.path.exists(success_file):\n","        print(\"[VOC2007] Found => skip download.\")\n","        return root_dir\n","    print(\"[VOC2007] Downloading train/val/test tar files from official site.\")\n","    os.makedirs(root_dir, exist_ok=True)\n","    tv_url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\"\n","    tst_url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\"\n","    tv_tar = os.path.join(root_dir, \"VOCtrainval_06-Nov-2007.tar\")\n","    tst_tar = os.path.join(root_dir, \"VOCtest_06-Nov-2007.tar\")\n","\n","    def dl(url, outpath):\n","        if os.path.exists(outpath):\n","            print(\"[VOC2007] Already downloaded =>\", outpath)\n","            return\n","        print(\"[VOC2007] Downloading =>\", url)\n","        r = requests.get(url, stream=True)\n","        r.raise_for_status()\n","        with open(outpath, \"wb\") as f:\n","            for chunk in r.iter_content(chunk_size=8192):\n","                f.write(chunk)\n","\n","    dl(tv_url, tv_tar)\n","    dl(tst_url, tst_tar)\n","\n","    def extract_tar(tfile, dest):\n","        print(\"[VOC2007] Extracting =>\", tfile)\n","        with tarfile.open(tfile, \"r\") as tf:\n","            tf.extractall(dest)\n","    extract_tar(tv_tar, root_dir)\n","    extract_tar(tst_tar, root_dir)\n","\n","    os.makedirs(os.path.dirname(success_file), exist_ok=True)\n","    with open(success_file, \"w\") as f:\n","        f.write(\"Done.\\n\")\n","\n","    return root_dir\n","\n","def fetch_pascal_context_official(root_dir=\"datasets/PascalContext_official\"):\n","    url = \"https://cs.stanford.edu/~roozbeh/pascal-context/trainval.tar.gz\"\n","    success = os.path.join(root_dir, \"VOCdevkit\", \"VOC2010\", \"_SUCCESS.txt\")\n","    if os.path.exists(success):\n","        print(\"[PascalContext] Found => skip download.\")\n","        return root_dir\n","    os.makedirs(root_dir, exist_ok=True)\n","    tar_fp = os.path.join(root_dir, \"trainval.tar.gz\")\n","    if not os.path.exists(tar_fp):\n","        print(\"[PascalContext] Downloading =>\", url)\n","        r = requests.get(url, stream=True)\n","        r.raise_for_status()\n","        with open(tar_fp, \"wb\") as f:\n","            for chunk in r.iter_content(chunk_size=8192):\n","                f.write(chunk)\n","    print(\"[PascalContext] Extracting =>\", tar_fp)\n","    with tarfile.open(tar_fp, \"r:gz\") as tf:\n","        tf.extractall(path=root_dir)\n","    Path(os.path.dirname(success)).mkdir(parents=True, exist_ok=True)\n","    with open(success, \"w\") as f:\n","        f.write(\"Done.\\n\")\n","    return root_dir\n","\n","def fetch_ade20k_via_hf(root_dir=\"datasets/ADE20K_HF\"):\n","    \"\"\"Fetch ADE20K from Hugging Face hub if not found locally.\"\"\"\n","    import datasets\n","    success_file = os.path.join(root_dir, \"_SUCCESS.txt\")\n","    if os.path.exists(success_file):\n","        print(\"[ADE20K] Found => skipping download.\")\n","        return root_dir\n","    os.makedirs(root_dir, exist_ok=True)\n","    print(\"[ADE20K] Downloading from huggingface.co 'zhoubolei/scene_parse_150'\")\n","    ds_all = datasets.load_dataset(\"zhoubolei/scene_parse_150\", split=None)\n","\n","    # Subdirs\n","    img_t_dir = os.path.join(root_dir, \"images\", \"training\")\n","    msk_t_dir = os.path.join(root_dir, \"annotations\", \"training\")\n","    img_v_dir = os.path.join(root_dir, \"images\", \"validation\")\n","    msk_v_dir = os.path.join(root_dir, \"annotations\", \"validation\")\n","    for d in [img_t_dir, msk_t_dir, img_v_dir, msk_v_dir]:\n","        os.makedirs(d, exist_ok=True)\n","\n","    for i, sample in enumerate(ds_all[\"train\"]):\n","        sample[\"image\"].save(os.path.join(img_t_dir, f\"train_{i:06d}.jpg\"))\n","        sample[\"annotation\"].save(os.path.join(msk_t_dir, f\"train_{i:06d}.png\"))\n","    for i, sample in enumerate(ds_all[\"validation\"]):\n","        sample[\"image\"].save(os.path.join(img_v_dir, f\"val_{i:06d}.jpg\"))\n","        sample[\"annotation\"].save(os.path.join(msk_v_dir, f\"val_{i:06d}.png\"))\n","\n","    with open(success_file, \"w\") as f:\n","        f.write(\"Done\\n\")\n","    return root_dir\n","\n","\n","########################################\n","# 2) PyTorch Dataset & Transforms (CPU-based or fallback)\n","########################################\n","def build_transforms(dataset_name=\"ade20k\", train=True, advanced_aug=False, normalization=\"none\"):\n","    short_size = 512\n","    t_list = []\n","    if train:\n","        t_list.append(T.RandomResizedCrop(short_size, scale=(0.5, 2.0)))\n","        t_list.append(T.RandomHorizontalFlip(0.5))\n","        if advanced_aug:\n","            t_list.append(T.ColorJitter(0.4, 0.4, 0.4, 0.2))\n","            t_list.append(T.RandomGrayscale(p=0.1))\n","    else:\n","        # For evaluation, resize then center crop to get fixed (512, 512) dimensions\n","        t_list.append(T.Resize(short_size))\n","        t_list.append(T.CenterCrop(short_size))\n","\n","    t_list.append(T.ToTensor())\n","\n","    if normalization.lower() == \"vit\":\n","        norm_mean, norm_std = [0.5]*3, [0.5]*3\n","        t_list.append(T.Normalize(mean=norm_mean, std=norm_std))\n","    elif normalization.lower() == \"imagenet\":\n","        t_list.append(T.Normalize(mean=[0.485, 0.456, 0.406],\n","                                  std=[0.229, 0.224, 0.225]))\n","    return T.Compose(t_list)\n","\n","\n","class SegmentationDataset(Dataset):\n","    \"\"\"\n","    A minimal example of a segmentation dataset loader that:\n","      - loads images & segmentation masks\n","      - can optionally cache in memory\n","      - applies transformations\n","    \"\"\"\n","    def __init__(self, dataset_name, split=\"train\", root_dir=None,\n","                 transform=None, target_transform=None, num_samples=50,\n","                 cache_data=False):\n","        super().__init__()\n","        self.dataset_name = dataset_name.lower()\n","        self.split = split.lower()\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.num_samples = num_samples\n","        self.cache_data = cache_data\n","\n","        self.img_paths = []\n","        self.mask_paths = []\n","\n","        if self.dataset_name == \"toy\":\n","            self.samples = [None]*num_samples\n","        elif self.dataset_name == \"cityscapes\":\n","            # cityscapes: {root}/leftImg8bit/{split}/{city}/*.png\n","            left_dir = os.path.join(root_dir, \"leftImg8bit\", self.split)\n","            gt_dir = os.path.join(root_dir, \"gtFine\", self.split)\n","            for city in sorted(os.listdir(left_dir)):\n","                city_img_path = os.path.join(left_dir, city)\n","                city_msk_path = os.path.join(gt_dir, city)\n","                if not os.path.isdir(city_img_path):\n","                    continue\n","                for fn in sorted(os.listdir(city_img_path)):\n","                    if fn.endswith(\"_leftImg8bit.png\"):\n","                        base = fn.replace(\"_leftImg8bit.png\", \"\")\n","                        mfn = base + \"_gtFine_labelIds.png\"\n","                        self.img_paths.append(os.path.join(city_img_path, fn))\n","                        self.mask_paths.append(os.path.join(city_msk_path, mfn))\n","        elif self.dataset_name == \"voc2007\":\n","            segfile = os.path.join(root_dir, \"VOCdevkit\", \"VOC2007\",\n","                                   \"ImageSets\", \"Segmentation\", f\"{self.split}.txt\")\n","            lines = []\n","            with open(segfile, \"r\") as f:\n","                lines = [l.strip() for l in f if l.strip()]\n","            for img_id in lines:\n","                jpg = os.path.join(root_dir, \"VOCdevkit\", \"VOC2007\", \"JPEGImages\", img_id + \".jpg\")\n","                msk = os.path.join(root_dir, \"VOCdevkit\", \"VOC2007\", \"SegmentationClass\", img_id + \".png\")\n","                self.img_paths.append(jpg)\n","                self.mask_paths.append(msk)\n","        elif self.dataset_name == \"ade20k\":\n","            if self.split == \"train\":\n","                self.img_dir = os.path.join(root_dir, \"images\", \"training\")\n","                self.msk_dir = os.path.join(root_dir, \"annotations\", \"training\")\n","            else:\n","                self.img_dir = os.path.join(root_dir, \"images\", \"validation\")\n","                self.msk_dir = os.path.join(root_dir, \"annotations\", \"validation\")\n","            imgs = sorted(os.listdir(self.img_dir))\n","            msks = sorted(os.listdir(self.msk_dir))\n","            for i, m in zip(imgs, msks):\n","                self.img_paths.append(os.path.join(self.img_dir, i))\n","                self.mask_paths.append(os.path.join(self.msk_dir, m))\n","        elif self.dataset_name == \"pascalcontext\":\n","            voc2010 = os.path.join(root_dir, \"VOCdevkit\", \"VOC2010\")\n","            seg_file = os.path.join(voc2010, \"ImageSets\", \"SegmentationContext\", f\"{self.split}.txt\")\n","            with open(seg_file, \"r\") as f:\n","                lines = [l.strip() for l in f if l.strip()]\n","            for img_id in lines:\n","                jpg = os.path.join(voc2010, \"JPEGImages\", img_id + \".jpg\")\n","                msk = os.path.join(voc2010, \"SegmentationClassContext\", img_id + \".png\")\n","                self.img_paths.append(jpg)\n","                self.mask_paths.append(msk)\n","\n","        # Optionally cache\n","        self.cached = False\n","        if self.cache_data and len(self.img_paths) > 0:\n","            self.cached = True\n","            self.cache = []\n","            from PIL import Image\n","            print(f\"[{dataset_name}] Caching {len(self.img_paths)} samples in memory...\")\n","            for i in range(len(self.img_paths)):\n","                try:\n","                    img = Image.open(self.img_paths[i]).convert(\"RGB\")\n","                    mask = Image.open(self.mask_paths[i])\n","                    self.cache.append((img, mask))\n","                except OSError as e:\n","                    print(f\"Warning: skipping corrupted image {self.img_paths[i]} => {e}\")\n","\n","    def __len__(self):\n","        if self.dataset_name == \"toy\":\n","            return len(self.samples)\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        from PIL import Image, ImageDraw\n","        import random\n","\n","        if self.dataset_name == \"toy\":\n","            # generate random shapes\n","            img = Image.new(\"RGB\", (128, 128), (0, 0, 0))\n","            mask = Image.new(\"L\", (128, 128), 0)\n","            draw = ImageDraw.Draw(img)\n","            shape = random.choice([\"rectangle\", \"ellipse\"])\n","            col = tuple(np.random.randint(0, 256, size=3))\n","            label = random.randint(1, 3)  # class label in [1..3]\n","            x1, y1 = np.random.randint(0, 64, size=2)\n","            x2 = x1 + np.random.randint(20, 64)\n","            y2 = y1 + np.random.randint(20, 64)\n","            if shape == \"rectangle\":\n","                draw.rectangle([x1, y1, x2, y2], fill=col)\n","                ImageDraw.Draw(mask).rectangle([x1, y1, x2, y2], fill=label)\n","            else:\n","                draw.ellipse([x1, y1, x2, y2], fill=col)\n","                ImageDraw.Draw(mask).ellipse([x1, y1, x2, y2], fill=label)\n","        else:\n","            # Load from disk or cache\n","            if self.cached:\n","                img, mask = self.cache[idx]\n","            else:\n","                from PIL import Image\n","                img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n","                mask = Image.open(self.mask_paths[idx])\n","\n","        if self.transform:\n","            img = self.transform(img)\n","        else:\n","            img = T.ToTensor()(img)\n","\n","        # By default, we match mask dims to image dims\n","        if self.target_transform:\n","            mask = self.target_transform(mask)\n","        else:\n","            mask = mask.resize((img.shape[2], img.shape[1]), resample=Image.NEAREST)\n","            mask_np = np.array(mask, dtype=np.int64)\n","            # For ADE20K official labeling: shift by -1, ignoring 0 => 255\n","            if self.dataset_name == \"ade20k\":\n","                # label 0 => ignore, 1..150 => 0..149\n","                mask_np = np.where(mask_np == 0, 255, mask_np - 1)\n","            mask = torch.from_numpy(mask_np)\n","        return img, mask\n","\n","########################################\n","# 3) Optional Mixup / CutMix for segmentation\n","########################################\n","def mixup_segmentation_naive(batch, alpha=0.2):\n","    \"\"\"Naive Mixup for segmentation: blend images; use random alpha to combine labels by choosing a random proportion.\"\"\"\n","    imgs, masks = zip(*batch)\n","    imgs = torch.stack(imgs, 0)\n","    masks = torch.stack(masks, 0)\n","    b = imgs.size(0)\n","    if b < 2:\n","        return imgs, masks\n","    idx = torch.randperm(b)\n","    lam = float(np.random.beta(alpha, alpha))\n","    mixed_imgs = lam * imgs + (1 - lam) * imgs[idx]\n","    # For segmentations, there's no single numeric mixing. We can mask out a fraction => approximate approach\n","    rand_mask = (torch.rand_like(masks.float()) < lam)\n","    mixed_masks = torch.where(rand_mask, masks, masks[idx])\n","    return mixed_imgs, mixed_masks\n","\n","def cutmix_segmentation_naive(batch, alpha=0.2):\n","    \"\"\"CutMix for segmentation: copy & paste a rectangle from another image’s region into current image & mask.\"\"\"\n","    imgs, masks = zip(*batch)\n","    imgs = torch.stack(imgs, 0)\n","    masks = torch.stack(masks, 0)\n","    b, c, h, w = imgs.shape\n","    if b < 2:\n","        return imgs, masks\n","    idx = torch.randperm(b)\n","    lam = float(np.random.beta(alpha, alpha))\n","\n","    rw = int(w * np.sqrt(1 - lam))\n","    rh = int(h * np.sqrt(1 - lam))\n","    rx = np.random.randint(0, w)\n","    ry = np.random.randint(0, h)\n","    x1 = np.clip(rx - rw // 2, 0, w)\n","    x2 = np.clip(rx + rw // 2, 0, w)\n","    y1 = np.clip(ry - rh // 2, 0, h)\n","    y2 = np.clip(ry + rh // 2, 0, h)\n","\n","    imgs[:, :, y1:y2, x1:x2] = imgs[idx, :, y1:y2, x1:x2]\n","    masks[:, y1:y2, x1:x2] = masks[idx, y1:y2, x1:x2]\n","    return imgs, masks\n","\n","########################################\n","# 4) (Optional) NVIDIA DALI pipeline for GPU-based data loading\n","########################################\n","try:\n","    from nvidia.dali.pipeline import Pipeline\n","    import nvidia.dali.fn as fn\n","    import nvidia.dali.types as types\n","    from nvidia.dali.plugin.pytorch import DALIGenericIterator\n","except ImportError:\n","    Pipeline = None  # If DALI not installed, fallback to CPU-based.\n","\n","class SegmentationTrainPipeline(Pipeline):\n","    def __init__(self, batch_size, num_threads, device_id,\n","                 file_root, mask_root, crop_size=512,\n","                 random_shuffle=True, seed=12):\n","        super(SegmentationTrainPipeline, self).__init__(batch_size, num_threads, device_id, seed=seed)\n","        self.input = fn.readers.file(file_root=file_root, random_shuffle=random_shuffle, seed=seed, name=\"Reader\")\n","        self.mask_input = fn.readers.file(file_root=mask_root, random_shuffle=random_shuffle, seed=seed, name=\"ReaderMask\")\n","        self.crop_size = crop_size\n","\n","    def define_graph(self):\n","        jpegs, _ = self.input()\n","        masks, _ = self.mask_input()\n","        images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.RGB)\n","        masks = fn.decoders.image(masks, device=\"cpu\", output_type=types.GRAY)\n","        images = fn.resize(images, resize_x=self.crop_size, resize_y=self.crop_size)\n","        masks = fn.resize(masks, resize_x=self.crop_size, resize_y=self.crop_size, interp_type=types.INTERP_NN)\n","        flip_coin = fn.random.coin_flip(probability=0.5)\n","        images = fn.flip(images, horizontal=flip_coin)\n","        masks = fn.flip(masks, horizontal=flip_coin)\n","        images = fn.brightness_contrast(images,\n","                                        brightness=fn.random.uniform(range=[0.8, 1.2]),\n","                                        contrast=fn.random.uniform(range=[0.8, 1.2]))\n","        return images, masks\n","\n","class SegmentationValPipeline(Pipeline):\n","    def __init__(self, batch_size, num_threads, device_id,\n","                 file_root, mask_root, crop_size=512, random_shuffle=False):\n","        super(SegmentationValPipeline, self).__init__(batch_size, num_threads, device_id)\n","        self.input = fn.readers.file(file_root=file_root, random_shuffle=random_shuffle, name=\"Reader\")\n","        self.mask_input = fn.readers.file(file_root=mask_root, random_shuffle=random_shuffle, name=\"ReaderMask\")\n","        self.crop_size = crop_size\n","\n","    def define_graph(self):\n","        jpegs, _ = self.input()\n","        masks, _ = self.mask_input()\n","        images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.RGB)\n","        masks = fn.decoders.image(masks, device=\"cpu\", output_type=types.GRAY)\n","        images = fn.resize(images, resize_x=self.crop_size, resize_y=self.crop_size)\n","        masks = fn.resize(masks, resize_x=self.crop_size, resize_y=self.crop_size, interp_type=types.INTERP_NN)\n","        return images, masks\n","\n","\n","########################################\n","# 5) Segmenter Model (ViT backbone, plus linear or mask-transformer decoder)\n","########################################\n","class MaskTransformerDecoder(nn.Module):\n","    \"\"\"\n","    Minimal mask transformer:\n","      - We store a class embedding for each of K classes.\n","      - We run 2 transformer-encoder layers on the concat of patch embeddings + class embeddings.\n","      - Then compute mask logits via patch_enc dot class_enc^T.\n","    \"\"\"\n","    def __init__(self, emb_dim, num_layers, num_heads, num_classes, dropout=0.1, drop_path_rate=0.0):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        # Class embeddings\n","        self.class_emb = nn.Parameter(torch.randn(num_classes, emb_dim))\n","        # Basic stack of transformer-encoder layers\n","        self.layers = nn.ModuleList([\n","            nn.TransformerEncoderLayer(\n","                d_model=emb_dim,\n","                nhead=num_heads,\n","                dim_feedforward=4 * emb_dim,\n","                dropout=dropout,\n","                activation='gelu',\n","                batch_first=False\n","            )\n","            for _ in range(num_layers)\n","        ])\n","        self.norm = nn.LayerNorm(emb_dim)\n","\n","    def forward(self, patch_tokens):\n","        B, N, E = patch_tokens.shape\n","        K = self.num_classes\n","        # Expand class embeddings => shape (B, K, E)\n","        class_embs = self.class_emb.unsqueeze(0).expand(B, K, E)\n","\n","        # Concat patch + class => shape (B, N+K, E)\n","        # but we pass them as seq-first => (N+K, B, E)\n","        x = torch.cat([patch_tokens, class_embs], dim=1)\n","        x = x.transpose(0, 1)\n","\n","        # pass through layers\n","        for layer in self.layers:\n","            x = layer(x)\n","\n","        x = self.norm(x)\n","        x = x.transpose(0, 1)\n","        # separate out patch_enc & class_enc\n","        patch_enc = x[:, :N, :]    # (B, N, E)\n","        class_enc = x[:, N:, :]    # (B, K, E)\n","        # L2 normalize\n","        patch_enc = F.normalize(patch_enc, dim=-1)\n","        class_enc = F.normalize(class_enc, dim=-1)\n","        # logits => (B, N, K)\n","        logits = torch.bmm(patch_enc, class_enc.transpose(1,2))\n","        return logits  # shape (B, N, K)\n","\n","class SegmenterModel(nn.Module):\n","    \"\"\"\n","    Based on timm Vision Transformer backbone, optionally with drop_path,\n","    and 2 possible decoders: 'linear' or 'mask'.\n","    \"\"\"\n","    def __init__(self,\n","                 backbone_name=\"vit_base_patch16_384\",\n","                 num_classes=150,\n","                 drop_path_rate=0.1,\n","                 decoder=\"mask\",\n","                 img_size=512,\n","                 backbone_dropout=0.0,\n","                 decoder_drop_path_rate=0.0,\n","                 distilled=False,\n","                 version=\"normal\"):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.decoder_type = decoder\n","        self.distilled = distilled\n","        self.version = version\n","\n","        # Create the ViT model from timm\n","        self.backbone = timm.create_model(\n","            backbone_name, pretrained=True,\n","            drop_path_rate=drop_path_rate,\n","            img_size=(img_size, img_size),\n","        )\n","        # Remove final classifier\n","        if hasattr(self.backbone, \"drop_rate\"):\n","            self.backbone.drop_rate = backbone_dropout\n","        if hasattr(self.backbone, \"head\"):\n","            self.backbone.head = nn.Identity()\n","        if hasattr(self.backbone, \"classifier\"):\n","            self.backbone.classifier = nn.Identity()\n","\n","        # some vit_xxx have \"forward_features\" or \"forward\"\n","        self.backbone.forward = self.backbone.forward_features\n","\n","        # figure out embedding dimension\n","        emb_dim = getattr(self.backbone, 'num_features', None)\n","        if emb_dim is None:\n","            # fallback for certain older timm ViT definitions\n","            emb_dim = self.backbone.embed_dim\n","\n","        # Create the decoder\n","        if decoder == \"linear\":\n","            # A simple 1x1 conv on patch embeddings => upsample to full\n","            self.seg_head = nn.Conv2d(emb_dim, num_classes, kernel_size=1)\n","        else:\n","            # Our mask transformer with 2 layers\n","            self.mask_transformer = MaskTransformerDecoder(\n","                emb_dim, num_layers=2, num_heads=12,\n","                num_classes=num_classes, dropout=0.1,\n","                drop_path_rate=decoder_drop_path_rate\n","            )\n","\n","    def forward(self, x):\n","        B, C, H, W = x.shape\n","        # forward backbone => shape (B, N, E)\n","        # typical for timm's ViT is (B, (1+N), E) if it includes a CLS token\n","        out = self.backbone(x)\n","        # out shape => (B, 1+N, E) if there's a CLS token\n","        # remove the CLS token if present\n","        seq_len = out.shape[1]\n","        if seq_len > (H*W)//(16*16)+1:\n","            # depends on patch size. Some timm models keep more tokens.\n","            out = out[:, 1:, :]  # remove CLS\n","        elif seq_len == (H*W)//(16*16)+1:\n","            out = out[:, 1:, :]\n","\n","        # Now => shape (B, N, E)\n","        B, N, E = out.shape\n","        # Suppose patch layout is sqrt(N) x sqrt(N)\n","        dpr = int(math.sqrt(N))\n","        # Ensure it is square\n","        if dpr*dpr != N:\n","            raise ValueError(\"Non-square patch layout => check your image/patch size alignment.\")\n","\n","        # (B, E, dpr, dpr)\n","        patch_tokens = out.transpose(1,2).reshape(B, E, dpr, dpr)\n","\n","        if self.decoder_type == \"linear\":\n","            # 1) linear decode\n","            logits_patch = self.seg_head(patch_tokens)\n","            # 2) upsample to (H, W)\n","            logits = F.interpolate(logits_patch, size=(H, W), mode='bilinear', align_corners=False)\n","            return logits\n","        else:\n","            # Mask transformer\n","            patch_seq = patch_tokens.flatten(2).transpose(1,2)  # => (B, N, E)\n","            logits_patch = self.mask_transformer(patch_seq)     # => (B, N, K)\n","            # shape => (B, K, N)\n","            logits_patch = logits_patch.transpose(1,2).reshape(B, self.num_classes, dpr, dpr)\n","            # upsample\n","            logits = F.interpolate(logits_patch, size=(H, W), mode='bilinear', align_corners=False)\n","            return logits\n","\n","\n","########################################\n","# 6) Optimizer, Scheduler, Evaluate\n","########################################\n","def poly_lr_scheduler(optimizer, base_lr, curr_iter, max_iter, power=0.9,\n","                      iter_warmup=0.0, min_lr=0.0):\n","    \"\"\"\n","    Poly LR schedule: lr = base_lr * (1 - t)^(power), with optional warmup\n","    \"\"\"\n","    if curr_iter < iter_warmup and iter_warmup > 0:\n","        lr = base_lr * (curr_iter / iter_warmup)\n","    else:\n","        effective_iter = curr_iter - iter_warmup if iter_warmup > 0 else curr_iter\n","        effective_max  = max_iter - iter_warmup if iter_warmup > 0 else max_iter\n","        lr = base_lr * (1 - effective_iter / effective_max) ** power\n","\n","    lr = max(lr, min_lr)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","def evaluate(model, loader, num_classes=150, ignore_index=255, device='cuda',\n","             im_size=512, window_size=512, window_stride=512):\n","    model.eval()\n","    total_correct = 0\n","    total_pixels = 0\n","    conf_mat = np.zeros((num_classes, num_classes), dtype=np.int64)\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            # Check if batch comes from DALI (dict) or a standard DataLoader (tensor tuple)\n","            if isinstance(batch, (list, tuple)) and isinstance(batch[0], dict):\n","                imgs = batch[0][\"img\"]\n","                masks = batch[0][\"mask\"]\n","            else:\n","                imgs, masks = batch\n","\n","            imgs = imgs.to(device)\n","            masks = masks.to(device)\n","\n","            logits = model(imgs)  # shape (B, K, H, W)\n","            preds = logits.argmax(dim=1)  # (B, H, W)\n","\n","            valid = (masks != ignore_index)\n","            total_correct += preds.eq(masks).logical_and(valid).sum().item()\n","            total_pixels += valid.sum().item()\n","\n","            # Confusion matrix\n","            p_cpu = preds[valid].cpu().numpy()\n","            m_cpu = masks[valid].cpu().numpy()\n","            combined = m_cpu * num_classes + p_cpu\n","            bin_count = np.bincount(combined, minlength=num_classes*num_classes)\n","            conf_mat += bin_count.reshape(num_classes, num_classes)\n","\n","    pix_acc = total_correct / (total_pixels + 1e-10)\n","    IoUs = []\n","    for c in range(num_classes):\n","        gt = conf_mat[c, :].sum()\n","        dt = conf_mat[:, c].sum()\n","        inter = conf_mat[c, c]\n","        union = gt + dt - inter\n","        if union > 0:\n","            IoUs.append(inter / union)\n","    mIoU = np.mean(IoUs) if len(IoUs) > 0 else 0\n","    model.train()\n","    return pix_acc, mIoU\n","\n","\n","\n","########################################\n","# 7) Main Training Loop\n","########################################\n","def main():\n","    import argparse\n","    # Filter out potential Jupyter args (like -f or .json)\n","    sys.argv = [arg for arg in sys.argv if not (arg.startswith('-f') or arg.endswith('.json'))]\n","\n","    parser = argparse.ArgumentParser()\n","    # Data & training schedule\n","    parser.add_argument(\"--dataset_name\", default=\"ade20k\",\n","                        choices=[\"toy\",\"ade20k\",\"voc2007\",\"cityscapes\",\"pascalcontext\"])\n","    parser.add_argument(\"--num_classes\", default=150, type=int)\n","    parser.add_argument(\"--batch_size\", default=32, type=int)\n","    parser.add_argument(\"--grad_accum_steps\", default=1, type=int)\n","    parser.add_argument(\"--num_epochs\", default=64, type=int)\n","    parser.add_argument(\"--start_epoch\", default=0, type=int)\n","    parser.add_argument(\"--eval_freq\", default=2, type=int)\n","    parser.add_argument(\"--total_iterations\", default=160000, type=int)  # e.g. 161728\n","    parser.add_argument(\"--power\", default=0.9, type=float)\n","    parser.add_argument(\"--iter_warmup\", default=0.0, type=float)\n","    parser.add_argument(\"--min_lr\", default=0.0, type=float)\n","    parser.add_argument(\"--clip_grad\", default=0.0, type=float)\n","    # Optim\n","    parser.add_argument(\"--lr\", default=0.001, type=float)\n","    parser.add_argument(\"--weight_decay\", default=0.0, type=float)\n","    parser.add_argument(\"--momentum\", default=0.9, type=float)\n","    # Model\n","    parser.add_argument(\"--model_name\", default=\"vit_base_patch16_384\")\n","    parser.add_argument(\"--stoch_depth\", default=0.1, type=float)\n","    parser.add_argument(\"--backbone_dropout\", default=0.0, type=float)\n","    parser.add_argument(\"--decoder\", default=\"mask\", choices=[\"linear\",\"mask\"])\n","    parser.add_argument(\"--decoder_drop_path_rate\", default=0.0, type=float)\n","    parser.add_argument(\"--distilled\", action=\"store_true\")\n","    parser.add_argument(\"--version\", default=\"normal\")\n","    parser.add_argument(\"--backbone_img_size\", default=512, type=int)\n","    # Augment\n","    parser.add_argument(\"--advanced_aug\", action=\"store_true\")\n","    parser.add_argument(\"--mixup\", action=\"store_true\")\n","    parser.add_argument(\"--cutmix\", action=\"store_true\")\n","    parser.add_argument(\"--dataset_normalization\", default=\"vit\")\n","    parser.add_argument(\"--cache_data\", action=\"store_true\")\n","    parser.add_argument(\"--resume\", action=\"store_true\", default=True)\n","    parser.add_argument(\"--debug\", action=\"store_true\", default=False)\n","    parser.add_argument(\"--ddp\", action=\"store_true\", default=False)\n","    parser.add_argument(\"--use_dali\", action=\"store_true\", default=False)\n","\n","    # Inference\n","    parser.add_argument(\"--inference_im_size\", default=512, type=int)\n","    parser.add_argument(\"--window_size\", default=512, type=int)\n","    parser.add_argument(\"--window_stride\", default=512, type=int)\n","\n","    args = parser.parse_args()\n","    if args.debug:\n","        print(\"Running in debug mode...\")\n","\n","    # Mount Google Drive\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Set checkpoint path to Google Drive\n","    checkpoint_path = \"/content/drive/MyDrive/your_folder/checkpoint.pth\"\n","    log_path = \"/content/drive/MyDrive/your_folder/training_logs.txt\"\n","\n","    print(f\"Using batch size {args.batch_size} and grad_accum={args.grad_accum_steps}.\")\n","\n","    ########################\n","    # DDP setup\n","    ########################\n","    if args.ddp:\n","        import torch.distributed as dist\n","        dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n","        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n","        device = torch.device(\"cuda\", local_rank)\n","        torch.cuda.set_device(device)\n","    else:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if device.type == \"cuda\":\n","        gpu_props = torch.cuda.get_device_properties(device)\n","        gpu_mem_gb = gpu_props.total_memory/(1024**3)\n","        print(f\"Detected GPU: {gpu_props.name} with total mem {gpu_mem_gb:.1f} GB\")\n","\n","    # auto enable caching if enough system RAM\n","    global free_ram_gb\n","    if not args.cache_data and free_ram_gb > 20:\n","        args.cache_data = True\n","        print(\"Automatically enabling data caching (cache_data=True) due to high available system RAM.\")\n","\n","    # fetch dataset if needed\n","    dsname = args.dataset_name.lower()\n","    if dsname == \"toy\":\n","        root = None\n","    elif dsname == \"ade20k\":\n","        root = fetch_ade20k_via_hf(\"datasets/ADE20K_HF\")\n","    elif dsname == \"cityscapes\":\n","        root = fetch_cityscapes_official(\"datasets/Cityscapes_official\")\n","    elif dsname == \"voc2007\":\n","        root = fetch_voc2007_official(\"datasets/VOC2007_official\")\n","    elif dsname == \"pascalcontext\":\n","        root = fetch_pascal_context_official(\"datasets/PascalContext_official\")\n","    else:\n","        raise ValueError(\"Unknown dataset:\", dsname)\n","\n","    # num_workers\n","    num_workers = min(cpu_physical, 16)\n","    print(f\"Setting DataLoader num_workers = {num_workers}\")\n","\n","    # Build or DALI\n","    if args.use_dali:\n","        if Pipeline is None:\n","            raise ImportError(\"Please install nvidia-dali to use --use_dali\")\n","        device_id = device.index if device.type=='cuda' else 0\n","        # Hard-coded for e.g. ADE20K layout\n","        train_file_root = os.path.join(root, \"images\", \"training\")\n","        train_mask_root = os.path.join(root, \"annotations\", \"training\")\n","        val_file_root   = os.path.join(root, \"images\", \"validation\")\n","        val_mask_root   = os.path.join(root, \"annotations\", \"validation\")\n","\n","        train_pipeline = SegmentationTrainPipeline(\n","            batch_size=args.batch_size,\n","            num_threads=num_workers,\n","            device_id=device_id,\n","            file_root=train_file_root,\n","            mask_root=train_mask_root,\n","            crop_size=args.backbone_img_size,\n","            random_shuffle=True)\n","        train_pipeline.build()\n","        train_loader = DALIGenericIterator(\n","            train_pipeline, output_map=[\"img\",\"mask\"],\n","            size=train_pipeline.epoch_size(\"Reader\"), auto_reset=True)\n","\n","        val_pipeline = SegmentationValPipeline(\n","            batch_size=1,\n","            num_threads=num_workers,\n","            device_id=device_id,\n","            file_root=val_file_root,\n","            mask_root=val_mask_root,\n","            crop_size=args.inference_im_size,\n","            random_shuffle=False)\n","        val_pipeline.build()\n","        val_loader = DALIGenericIterator(\n","            val_pipeline, output_map=[\"img\",\"mask\"],\n","            size=val_pipeline.epoch_size(\"Reader\"),\n","            auto_reset=True)\n","    else:\n","        # standard PyTorch dataset\n","        train_tf = build_transforms(dsname, train=True,\n","                                    advanced_aug=args.advanced_aug,\n","                                    normalization=args.dataset_normalization)\n","        val_tf   = build_transforms(dsname, train=False,\n","                                    advanced_aug=False,\n","                                    normalization=args.dataset_normalization)\n","\n","        if dsname==\"toy\":\n","            train_ds = SegmentationDataset(\"toy\", \"train\",\n","                                           transform=train_tf,\n","                                           cache_data=args.cache_data)\n","            val_ds   = SegmentationDataset(\"toy\", \"val\",\n","                                           transform=val_tf,\n","                                           cache_data=args.cache_data)\n","        else:\n","            train_ds = SegmentationDataset(dsname, \"train\", root,\n","                                           transform=train_tf,\n","                                           cache_data=args.cache_data)\n","            val_ds   = SegmentationDataset(dsname, \"val\", root,\n","                                           transform=val_tf,\n","                                           cache_data=args.cache_data)\n","\n","        if args.ddp:\n","            train_sampler = torch.utils.data.distributed.DistributedSampler(train_ds)\n","            val_sampler   = torch.utils.data.distributed.DistributedSampler(val_ds, shuffle=False)\n","        else:\n","            train_sampler, val_sampler = None, None\n","\n","        train_loader = DataLoader(train_ds, batch_size=args.batch_size,\n","                                  shuffle=(train_sampler is None),\n","                                  sampler=train_sampler, drop_last=True,\n","                                  num_workers=num_workers, pin_memory=True,\n","                                  persistent_workers=True, prefetch_factor=4)\n","\n","        val_loader = DataLoader(val_ds, batch_size=1, shuffle=False,\n","                                sampler=val_sampler, drop_last=False,\n","                                num_workers=num_workers, pin_memory=True,\n","                                persistent_workers=True, prefetch_factor=4)\n","\n","    # Build model\n","    model = SegmenterModel(\n","        backbone_name=args.model_name,\n","        num_classes=args.num_classes,\n","        drop_path_rate=args.stoch_depth,\n","        decoder=args.decoder,\n","        img_size=args.backbone_img_size,\n","        backbone_dropout=args.backbone_dropout,\n","        decoder_drop_path_rate=args.decoder_drop_path_rate,\n","        distilled=args.distilled,\n","        version=args.version\n","    ).to(device)\n","\n","    # channels_last if GPU\n","    if device.type == \"cuda\":\n","        model = model.to(memory_format=torch.channels_last)\n","\n","    # DDP\n","    if args.ddp:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model, device_ids=[device.index] if device.type=='cuda' else None\n","        )\n","\n","    # Optional compile (PyTorch 2.0+)\n","    if hasattr(torch, 'compile'):\n","        model = torch.compile(model)\n","\n","    # Optim\n","    optimizer = torch.optim.SGD(model.parameters(),\n","                                lr=args.lr,\n","                                momentum=args.momentum,\n","                                weight_decay=args.weight_decay)\n","    # AMP\n","    scaler = torch.cuda.amp.GradScaler() if device.type=='cuda' else None\n","\n","    # Possibly resume\n","    global_step = 0\n","    best_mIoU = 0.0\n","    start_epoch = args.start_epoch\n","    # checkpoint_path = \"checkpoint.pth\"\n","    if args.resume and os.path.isfile(checkpoint_path):\n","        rank0 = (not args.ddp) or (args.ddp and torch.distributed.get_rank()==0)\n","        if rank0:\n","            print(\"Resuming from checkpoint.pth ...\")\n","        ckpt = torch.load(checkpoint_path, map_location=device)\n","        model.load_state_dict(ckpt[\"model_state_dict\"])\n","        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n","        global_step = ckpt.get(\"global_step\", 0)\n","        start_epoch = ckpt.get(\"epoch\", 0)\n","        best_mIoU   = ckpt.get(\"best_mIoU\", 0.0)\n","\n","    # Train\n","    rank0 = (not args.ddp) or (args.ddp and torch.distributed.get_rank()==0)\n","    if rank0:\n","        print(f\"Start training for {args.num_epochs} epochs (total iterations: {args.total_iterations}).\")\n","\n","    for epoch in range(start_epoch, args.num_epochs):\n","        if args.ddp and (not args.use_dali):\n","            train_loader.sampler.set_epoch(epoch)\n","\n","        model.train()\n","        epoch_loss = 0.0\n","        num_batches = 0\n","\n","        iterator = train_loader if not rank0 else tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False)\n","        for batch in iterator:\n","            if args.use_dali:\n","                # DALI returns list of dicts\n","                imgs = batch[0][\"img\"]\n","                masks = batch[0][\"mask\"]\n","            else:\n","                imgs, masks = batch\n","\n","            imgs = imgs.to(device, memory_format=torch.channels_last)\n","            masks = masks.to(device)\n","\n","            # optional mixup/cutmix\n","            if args.mixup and args.cutmix:\n","                raise ValueError(\"Pick either --mixup or --cutmix, not both.\")\n","            if args.mixup:\n","                imgs, masks = mixup_segmentation_naive(list(zip(imgs, masks)), alpha=0.2)\n","            if args.cutmix:\n","                imgs, masks = cutmix_segmentation_naive(list(zip(imgs, masks)), alpha=0.2)\n","\n","            # set LR\n","            poly_lr_scheduler(optimizer, args.lr, global_step, args.total_iterations,\n","                              power=args.power, iter_warmup=args.iter_warmup, min_lr=args.min_lr)\n","\n","            # forward + backward\n","            with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n","                logits = model(imgs)\n","                loss = nn.CrossEntropyLoss(ignore_index=255)(logits, masks) / args.grad_accum_steps\n","\n","            if scaler is not None:\n","                scaler.scale(loss).backward()\n","            else:\n","                loss.backward()\n","\n","            epoch_loss += loss.item()*args.grad_accum_steps\n","            num_batches += 1\n","            global_step += 1\n","\n","            if rank0 and isinstance(iterator, tqdm):\n","                iterator.set_postfix({\n","                    'loss': f\"{loss.item()*args.grad_accum_steps:.4f}\",\n","                    'lr':   f\"{optimizer.param_groups[0]['lr']:.6f}\"\n","                })\n","\n","            if global_step % args.grad_accum_steps == 0:\n","                if args.clip_grad>0:\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","                if scaler is not None:\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","                else:\n","                    optimizer.step()\n","                optimizer.zero_grad()\n","\n","            if global_step >= args.total_iterations:\n","                break\n","\n","        avg_loss = epoch_loss / max(num_batches, 1)\n","        if rank0:\n","            print(f\"Epoch {epoch} completed. Avg Loss: {avg_loss:.4f}\")\n","\n","        # Evaluate\n","        if ((epoch+1)%args.eval_freq==0 or (epoch == args.num_epochs-1)) and rank0:\n","            pixacc, miou = evaluate(model, val_loader,\n","                                    num_classes=args.num_classes,\n","                                    ignore_index=255,\n","                                    device=device,\n","                                    im_size=args.inference_im_size,\n","                                    window_size=args.window_size,\n","                                    window_stride=args.window_stride)\n","            print(f\"Evaluation at epoch {epoch}: pixAcc={pixacc:.4f}  mIoU={miou:.4f}\")\n","            # save best\n","            if miou > best_mIoU:\n","                best_mIoU = miou\n","                print(f\"New best mIoU={best_mIoU:.4f} => saving checkpoint.\")\n","                torch.save({\n","                    \"epoch\": epoch,\n","                    \"global_step\": global_step,\n","                    \"model_state_dict\": model.state_dict(),\n","                    \"optimizer_state_dict\": optimizer.state_dict(),\n","                    \"best_mIoU\": best_mIoU,\n","                }, checkpoint_path)\n","\n","        if global_step >= args.total_iterations:\n","            if rank0:\n","                print(\"Reached total iteration limit. Stopping.\")\n","            break\n","\n","    if rank0:\n","        print(\"Training complete!\")\n","        print(f\"Best mIoU => {best_mIoU:.4f}\")\n","        print(\"Done.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzufvH9RcHG9","outputId":"d3d525be-74e1-4203-d654-6bd308d7a1c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected CPU: 12 logical cores, 6 physical cores.\n","Available system RAM: 36.9 GB\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using batch size 32 and grad_accum=1.\n","Detected GPU: NVIDIA A100-SXM4-40GB with total mem 39.6 GB\n","Automatically enabling data caching (cache_data=True) due to high available system RAM.\n","[ADE20K] Found => skipping download.\n","Setting DataLoader num_workers = 6\n","[ade20k] Caching 20210 samples in memory...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BHrdfYyZV9tz"},"execution_count":null,"outputs":[]}]}